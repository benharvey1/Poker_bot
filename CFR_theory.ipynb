{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CFR\n",
    "\n",
    "Poker is a game of imperfect information (you do not know the opponents cards). Games like this can be represented by a game tree where each node in the tree either represents a player's decision, a chance event (e.g. flop is dealt) or a terminal outcome (e.g. showdown) and the edges represent the actions taken (e.g. check, bet, call etc.). \n",
    "<br><br> Each decision node (referred to as an information set) is defined by the active player and all the information available to the player at that point in the game (i.e their own cards and the game history). Information sets contain multiple game states which the player percieves as indistinguishable based on the information available to them. \n",
    "<br>Let $I$ denote an _information set_ and let $A(I)$ be the set of legal actions for that information set. At each information set, we wish to determine the probability player $i$ should choose action $a \\in A(I)$, $\\sigma_i(I,a)$, to maximise their expected payoff. The way we find this is using an algorithm called counterfactual regret minimisation, which uses positive regret matching to determine the best strategy. We introduce some notation and the general idea of the algorithm below.\n",
    "<br><br> A _history_ $h$ is a sequence of actions starting from the root of the tree (here we don't include chance events). In conventional poker games the game history is always known to both players. Therefore, each information set only contains a single history (but contains multiple game states since opponent cards are not known).  Let $\\Pi^{\\sigma}(I)$ be the probability of reaching an information set $I$, while following strategy $\\sigma$. \n",
    "The reach probability can be decomposed into the individual contributions of each player. For two player games:\n",
    "$$\\Pi^{\\sigma}(I) = \\Pi^{\\sigma}_{1}(I)\\Pi^{\\sigma}_2(I)$$\n",
    "Let $Z$ denote the set of all terminal game histories (i.e. sequences from root to leaf) and let $u_{i}(z)$ denote the utility (payoff) of player $i$ for terminal game history $z\\in Z$. The _counterfactual value_ $\\nu_i(\\sigma, I)$ (hypothetical payoff) for player $i$ at some information set $I$ with strategy $\\sigma$, is given by:\n",
    "<br><br>$$ \\nu_i(\\sigma, I) = \\sum\\limits_{z\\in Z} \\Pi^{\\sigma}_{-i}(I)\\Pi^{\\sigma}_{i}(z|I)u_i(z) $$\n",
    "where $\\Pi^{\\sigma}_{-i}(I)$, is the probability of reaching information set $I$ with strategy profile $\\sigma$, while not including the contribution from player $i$ (this is why it is a counterfactual value). The idea is to compute this at every information set in the game tree.\n",
    "Once this is computed, one can compute the regret for player $i$ of not taking action $a$ at information set $I$:\n",
    "$$ r_i(a, I) = \\nu_i(\\sigma_{I\\rightarrow a}, I) - \\nu_i(\\sigma, I) $$\n",
    "where $\\sigma_{I\\rightarrow a}$ denotes that action $a$ is chosen at information set $I$. Therefore the regret of not choosing action $a$ is defined as the difference between the hypothetical payoff of choosing action $a$ and the hypothetical payoff from following the current strategy.\n",
    "<br><br> The cumulative regret of this action is then given by:\n",
    "$$ R^{T}_{i}(I,a) = \\sum\\limits_{t=1}^{T} r^{t}_{i}(I,a) $$\n",
    "Where the sum is over all instances which information set $I$ is visited.\n",
    "The cumulative regrets are then used to obtain the strategy for the next time information set $I$ is visited using regret matching:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\sigma^{T+1}_{i}(I,a) = \\left\\{\n",
    "\\begin{array}{rcl}\n",
    "\\frac{\\max(R^{T}_{i}(I,a), 0)}{\\sum\\limits_{a\\in A(I)} \\max(R^{T}_{i}(I,a), 0)} & \\text{if} & \\sum\\limits_{a\\in A(I)} \\max(R^{T}_{i}(I,a), 0) > 0 \\\\\n",
    "\\frac{1}{|A(I)|} & & \\text{otherwise}\n",
    "\\end{array}\\right.\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The _average_ strategy at information set $I$, not the final strategy, approaches an equilibrium as $T \\rightarrow \\infty $.\n",
    "\n",
    "In this project we use a particular variant of the CFR algorithm called Chance sampling Monte Carlo CFR (MCCFR). This selects a single chance node at the root of the tree for each iteration (equivalent to selecting a specific dealing of the cards). The advantage of this is that only part of the game tree is explored each iteration, leading to faster convergence to Nash Equilibrium (especially for larger games). \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
